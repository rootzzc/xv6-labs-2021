diff --git a/kernel/defs.h b/kernel/defs.h
index 3564db4..33452af 100644
--- a/kernel/defs.h
+++ b/kernel/defs.h
@@ -170,6 +170,8 @@ uint64          walkaddr(pagetable_t, uint64);
 int             copyout(pagetable_t, uint64, char *, uint64);
 int             copyin(pagetable_t, char *, uint64, uint64);
 int             copyinstr(pagetable_t, char *, uint64, uint64);
+int             uvmcow(pagetable_t, pagetable_t, uint64);
+pte_t *         walk(pagetable_t, uint64, int);
 
 // plic.c
 void            plicinit(void);b
diff --git a/kernel/kalloc.c b/kernel/kalloc.c
index fa6a0ac..9c942ad 100644
--- a/kernel/kalloc.c
+++ b/kernel/kalloc.c
@@ -14,6 +14,7 @@ void freerange(void *pa_start, void *pa_end);
 extern char end[]; // first address after kernel.
                    // defined by kernel.ld.
 
+uint64 pa_ref[(PHYSTOP - KERNBASE) / PGSIZE];  // indicate number of proc ref to physical page
 struct run {
   struct run *next;
 };
@@ -51,6 +52,13 @@ kfree(void *pa)
   if(((uint64)pa % PGSIZE) != 0 || (char*)pa < end || (uint64)pa >= PHYSTOP)
     panic("kfree");
 
+  acquire(&kmem.lock);
+  if(--pa_ref[(uint64)pa / PGSIZE] > 0){
+    release(&kmem.lock);
+    return;
+  }
+  release(&kmem.lock);
+
   // Fill with junk to catch dangling refs.
   memset(pa, 1, PGSIZE);
 
@@ -74,6 +82,7 @@ kalloc(void)
   r = kmem.freelist;
   if(r)
     kmem.freelist = r->next;
+  pa_ref[PGROUNDDOWN((uint64)r) / PGSIZE] = 1;
   release(&kmem.lock);
 
   if(r)
diff --git a/kernel/proc.c b/kernel/proc.c
index 22e7ce4..ec395ce 100644
--- a/kernel/proc.c
+++ b/kernel/proc.c
@@ -282,7 +282,7 @@ fork(void)
   }
 
   // Copy user memory from parent to child.
-  if(uvmcopy(p->pagetable, np->pagetable, p->sz) < 0){
+  if(uvmcow(p->pagetable, np->pagetable, p->sz) < 0){
     freeproc(np);
     release(&np->lock);
     return -1;
diff --git a/kernel/riscv.h b/kernel/riscv.h
index 1691faf..bb135e0 100644
--- a/kernel/riscv.h
+++ b/kernel/riscv.h
@@ -343,6 +343,7 @@ sfence_vma()
 #define PTE_W (1L << 2)
 #define PTE_X (1L << 3)
 #define PTE_U (1L << 4) // 1 -> user can access
+#define PTE_COW (1L << 8) // copy on write
 
 // shift a physical address to the right place for a PTE.
 #define PA2PTE(pa) ((((uint64)pa) >> 12) << 10)
diff --git a/kernel/trap.c b/kernel/trap.c
index a63249e..f56ab1d 100644
--- a/kernel/trap.c
+++ b/kernel/trap.c
@@ -10,6 +10,7 @@ struct spinlock tickslock;
 uint ticks;
 
 extern char trampoline[], uservec[], userret[];
+extern uint64 pa_ref[(PHYSTOP - KERNBASE) / PGSIZE];  // kalloc.c
 
 // in kernelvec.S, calls kerneltrap().
 void kernelvec();
@@ -65,6 +66,30 @@ usertrap(void)
     intr_on();
 
     syscall();
+  } else if(r_scause() == 15){
+    // pte_t *pte;
+    // uint64 va;
+    // uint64 pa;
+    // uint flags;
+    // void *new_pa;
+    // va = r_stval();
+    // if((pte = walk(p->pagetable, va, 0)) == 0 || (*pte & PTE_COW) == 0){
+    //   panic("write: tpe invalid page to write");
+    // }
+    // // va is valid virtual address and cow physical address
+    // pa = PTE2PA(*pte);
+    // new_pa = kalloc();
+    // memmove(new_pa, (char*)pa, PGSIZE);
+    // // change pte to write valid
+    // *pte &= ~PTE_V;  // avoid remap panic from mappages
+    // flags = PTE_FLAGS(*pte);
+    // flags |= PTE_W;
+    // flags &= ~PTE_COW;
+    // if(mappages(p->pagetable, va, PGSIZE, (uint64)new_pa, flags) != 0){
+    //   kfree(new_pa);
+    //   return;
+    // }
+    // pa_ref[(uint64)new_pa / PGSIZE]++;
   } else if((which_dev = devintr()) != 0){
     // ok
   } else {
diff --git a/kernel/vm.c b/kernel/vm.c
index d5a12a0..13ca38f 100644
--- a/kernel/vm.c
+++ b/kernel/vm.c
@@ -15,6 +15,8 @@ extern char etext[];  // kernel.ld sets this to end of kernel code.
 
 extern char trampoline[]; // trampoline.S
 
+extern uint64 pa_ref[(PHYSTOP - KERNBASE) / PGSIZE];  // kalloc.c
+
 // Make a direct-map page table for the kernel.
 pagetable_t
 kvmmake(void)
@@ -432,3 +434,49 @@ copyinstr(pagetable_t pagetable, char *dst, uint64 srcva, uint64 max)
     return -1;
   }
 }
+
+// copy on write
+int
+uvmcow(pagetable_t old, pagetable_t new, uint64 sz)
+{
+  pte_t *pte;
+  uint64 pa, i;
+  uint flags;
+
+  for(i = 0; i < sz; i += PGSIZE){
+    if((pte = walk(old, i, 0)) == 0)
+      panic("uvmcopy: pte should exist");
+    if((*pte & PTE_V) == 0)
+      panic("uvmcopy: page not present");
+    pa = PTE2PA(*pte);
+    pte_t npte = *pte;
+
+    // if old pte can write, set cow flag and clear write flag
+    if(*pte & PTE_W){
+      npte |= PTE_COW;
+      npte &= ~PTE_W;
+    }
+    flags = PTE_FLAGS(npte);
+
+    if(mappages(new, i, PGSIZE, pa, flags) != 0){
+      goto err;
+    }
+  }
+
+  // no error happened, change origined pte
+  for(i = 0; i < sz; i += PGSIZE){
+    pte = walk(old, i, 0);
+    if(*pte & PTE_W){
+      *pte |= PTE_COW;
+      *pte &= ~PTE_W;
+    }
+    pa = PTE2PA(*pte);
+    pa_ref[PGROUNDDOWN(pa) / PGSIZE]++;
+  }
+
+  return 0;
+
+ err:
+  uvmunmap(new, 0, i / PGSIZE, 0);
+  return -1;
+}
\ No newline at end of file
